// llm-ping â€” ÑƒÑ‚Ð¸Ð»Ð¸Ñ‚Ð° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚Ð¸ LLM Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°.
//
// Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ:
//   go run cmd/llm-ping/main.go
//   Ð¸Ð»Ð¸
//   go build -o llm-ping cmd/llm-ping/main.go && ./llm-ping
//
// ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ:
//   OPENROUTER_API_KEY - API ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ OpenRouter
//   ZAI_API_KEY        - API ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ Zai
//   OPENAI_API_KEY     - API ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ OpenAI
//
// ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ:
//   Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ config.yaml Ð¸Ð· Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"time"

	"github.com/ilkoid/poncho-ai/pkg/config"
	"github.com/ilkoid/poncho-ai/pkg/models"
	"github.com/ilkoid/poncho-ai/pkg/tools/std"
)

func main() {
	// 1. Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ
	cfgPath := "config.yaml"
	if len(os.Args) > 1 {
		cfgPath = os.Args[1]
	}

	cfg, err := config.Load(cfgPath)
	if err != nil {
		log.Fatalf("Failed to load config from %s: %v", cfgPath, err)
	}

	// 2. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ModelRegistry
	modelRegistry, err := models.NewRegistryFromConfig(cfg)
	if err != nil {
		log.Fatalf("Failed to create model registry: %v", err)
	}

	// 3. Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ LLM Ping Tool
	toolCfg, exists := cfg.Tools["ping_llm_provider"]
	if !exists {
		fmt.Println("âŒ ping_llm_provider tool not found in config.yaml")
		fmt.Println("\nAdd this to your config.yaml:")
		fmt.Println(`
tools:
  ping_llm_provider:
    enabled: true
    description: "ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ LLM Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°"
`)
		os.Exit(1)
	}

	pingTool := std.NewLLMPingTool(modelRegistry, cfg, toolCfg)

	// 4. ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´ÐµÑ„Ð¾Ð»Ñ‚Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð· ÐºÐ¾Ð½Ñ„Ð¸Ð³Ð°
	modelAlias := cfg.Models.DefaultChat
	if modelAlias == "" {
		fmt.Println("âš ï¸  No default_chat model configured in config.yaml")
		fmt.Println("Testing first available model...")

		// Ð‘ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²ÑƒÑŽ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
		for name := range cfg.Models.Definitions {
			modelAlias = name
			break
		}
	}

	fmt.Printf("ðŸ” Testing LLM Provider: %s\n\n", modelAlias)

	// 5. Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ping
	ctx, cancel := context.WithTimeout(context.Background(), 15*time.Second)
	defer cancel()

	args := ""
	if modelAlias != "" {
		argsJSON, _ := json.Marshal(map[string]string{"model": modelAlias})
		args = string(argsJSON)
	}

	result, err := pingTool.Execute(ctx, args)
	if err != nil {
		log.Fatalf("Failed to execute ping: %v", err)
	}

	// 6. ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
	var pingResult map[string]interface{}
	if err := json.Unmarshal([]byte(result), &pingResult); err != nil {
		fmt.Printf("Raw result: %s\n", result)
		return
	}

	// ÐšÑ€Ð°ÑÐ¸Ð²Ñ‹Ð¹ Ð²Ñ‹Ð²Ð¾Ð´
	printResult(pingResult)
}

// printResult Ð²Ñ‹Ð²Ð¾Ð´Ð¸Ñ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¿Ð¸Ð½Ð³Ð° Ð² ÐºÑ€Ð°ÑÐ¸Ð²Ð¾Ð¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ
func printResult(result map[string]interface{}) {
	available, _ := result["available"].(bool)
	statusCode, _ := result["status_code"].(float64)
	latencyMs, _ := result["latency_ms"].(float64)
	provider, _ := result["provider"].(string)
	model, _ := result["model"].(string)

	if available {
		fmt.Printf("âœ… Status: AVAILABLE\n")
		fmt.Printf("   Provider: %s\n", provider)
		fmt.Printf("   Model: %s\n", model)
		fmt.Printf("   Latency: %dms\n", int(latencyMs))
		if statusCode > 0 {
			fmt.Printf("   HTTP Code: %d\n", int(statusCode))
		}
		if msg, ok := result["message"].(string); ok {
			fmt.Printf("   Message: %s\n", msg)
		}
	} else {
		fmt.Printf("âŒ Status: UNAVAILABLE\n")
		if provider != "" {
			fmt.Printf("   Provider: %s\n", provider)
		}
		if model != "" {
			fmt.Printf("   Model: %s\n", model)
		}
		if errType, ok := result["error_type"].(string); ok {
			fmt.Printf("   Error Type: %s\n", errType)
		}
		if errMsg, ok := result["error"].(string); ok {
			fmt.Printf("   Error: %s\n", errMsg)
		}
		if statusCode > 0 {
			fmt.Printf("   HTTP Code: %d\n", int(statusCode))
		}
		if latencyMs > 0 {
			fmt.Printf("   Latency: %dms\n", int(latencyMs))
		}
	}
}
